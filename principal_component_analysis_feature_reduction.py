# -*- coding: utf-8 -*-
"""Principal_Component_Analysis_Feature_Reduction (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ogzHZPmWG9sQVAmfF5QFdB2WEK9wD8ud

**PCA is used to speed up a reasonable choice. PCA allows you to save space in exchange for losing a little of the data's information which could be tradefoff, but not necessaritly if the chosen variables contain over 85% of the variance.**

**Extremely useful for visualizing big data and saving memory space in a system as variables with a cumulative variance of less than 15% can be eliminiated.**

**Importing Libraries**
"""

from sklearn import datasets #sklearn.datasets package embeds some small toy datasets as introduced in the Getting Started section
import pandas as pd #datavisualization library
import matplotlib.pyplot as plt #makes matplotlib works like MATLAB
import numpy as np #library provides support for n dimensional arrays
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler #to standardize features
from sklearn.decomposition import PCA

"""**Loading the IRIS Dataset**"""

iris = datasets.load_iris()
X = iris.data
X.shape
Y= iris.target
df = pd.DataFrame (X, columns =['sepal length','sepal width','petal length','petal width'])
df_target = pd.DataFrame(Y, columns= ['target'])
print (df)
print (df_target)

"""**Standardizing the Dataset**



Standardization would shift each attribute to have a mean of 0 and a variance of 1. It's a common requirement of models to avoid any bias.
"""

normalized_X= StandardScaler().fit_transform(X)
pca= PCA(n_components=2)

#this returns and prints low dim matrix
def my_pca(data_matrix, k):
  pca = PCA(n_components=2) #reduce to 2 dimensions
  principalComponent = pca.fit_transform(normalized_X) 
  principal_Data_Frame = pd.DataFrame(data=principalComponent, columns= ['Principal Component 1','Principal Component 2'])
  finalDf= pd.concat ([principal_Data_Frame, df_target], axis=1)
  return finalDf

"""**Plotting the DataSet**"""

def my_pca_plot(finalDataFrame):
  fig = plt.figure(figsize = (12,8))
  targets = [0, 1, 2]
  for target in targets:
    filtered_df = finalDataFrame.loc[(finalDataFrame.target == target),:]
    x_val = np.array(filtered_df['Principal Component 1'])
    y_val = np.array(filtered_df['Principal Component 2'])
    if target == 0:
      plt.scatter(x_val, y_val, label = 'Iris-setosa')
    elif target == 1:
      plt.scatter(x_val, y_val, label = 'Iris-versicolor')
    else:
      plt.scatter(x_val, y_val, label = 'Iris-virginica')
  plt.xlabel('Principal Component 1')
  plt.ylabel('Principal Component 2')
  plt.title('2-Component-PCA')
  plt.legend()
  plt.show()

"""**PCA for obtaining (150X2) matrix and getting a boxplot**"""

#calling the first function
from sklearn.decomposition import PCA
low_dim = my_pca(normalized_X, 2)
print (low_dim)
my_pca_plot(low_dim)

"""**Explaining Variance**

**By *explained variance ratio*, the 2 variable should contain atleast 85% of the information after feature reduction for the model to have some meaning.**

Together, the first two principal components contain **85.17%** of the information. The first principal component contains 64.86% of the variance and the second principal component contains 20.31% of the variance. The third and fourth principal component contained the rest of the variance of the dataset. 

The two principle components contain the threshold value. Hence, we know the right features have been chosen.
"""

expl_var_pca = np.var(my_pca(normalized_X, 2), axis=0)
print('explained variance pca: ', expl_var_pca)
expl_var_ratio_pca = expl_var_pca / np.sum(expl_var_pca)

print('explained variance ratio pca: ', expl_var_ratio_pca)

